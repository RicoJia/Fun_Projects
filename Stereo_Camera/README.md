# From Ground Zero to 3D Point Cloud Using Stereo Camera
3D point cloud is widely used in autonomous driving, construction, augmented reality, etc. The 3D point cloud is best generated by a 3D Lidar. However for regular hobbyist projects, RGBD cameras, even a stereo camera, can give us some good estimations of the 3D environment. 

In this article, I am going to demonstrate this process using a dual lens stereo camera: 
    <p align="center">
    <img src="https://user-images.githubusercontent.com/39393023/149461165-9beba72c-421c-4570-a25a-9d6fa6e7b230.png" height="400" width="width"/>
    <figcaption align="center">SVPRO dual lens camera</figcaption>
    </p>

The major steps include: calibrating each lens as a single camera, calibrate two lenses as a stereo system, disparity image generation, and finally, estimating the 3D position of each pixel on the disparity image. 
    <p align="center">
    <img src="https://user-images.githubusercontent.com/39393023/149460886-7ce3a75c-ad28-4818-a088-ac1586cdf33f.png" height="800" width="width"/>
    <figcaption align="center">General code work flow - calibration and depth image</figcaption>
    </p>

# Single Camera Calibration 
Before stereo calibration, first we need to calibrate each lens. There are different types of camera models, such as for a fish eye camera (with large FOV), a good model is the [MEI](https://www.robots.ox.ac.uk/~cmei/articles/single_viewpoint_calib_mei_07.pdf), currently adopted by [OpenCV](https://docs.opencv.org/3.4/dd/d12/tutorial_omnidir_calib_main.html). In this article, we are going to use the traditional pinhole camera model. 

## Pinhole Model 
The common pinhole model is set up as below. $f$ is focal length. The image will be projected onto the plane behind the lens, in an angle $\phi$
      <p align="center">
      <img src="https://user-images.githubusercontent.com/39393023/121839683-3d499c00-cca0-11eb-8faf-22485a5248e1.png" height="200" width="Field of View"/>
      <figcaption align="center">Pinhole Model. Credit: Udacity</figcaption>
      </p>

In the above depiction, note that the projection of an object through pinhole is upside down. For convenience, in 3D we shift the image plane **before** the pinhole, and we set up the camera frame at the pinhole. This way, camera frame coordinates on the image plane can be easily calculated from the 3D world frame coordinates using the similar triangle method, without flipping them upside down. 
    <p align="center">
    <img src="https://docs.opencv.org/4.x/pinhole_camera_model.png" height="200" width="width"/>
    <figcaption align="center">3D Pinhole Model. Credit: OpenCV</figcaption>
    </p>

Here we usually represent coordinates using homogenous coordinates, which allows us to represent depth ```z``` very easily in matrix form. This is important because the same point on the image plane can be the projection of points of different depth.    

1. The model of a simple pinhole camera can be written as:  ```world frame -> camera frame -> pixel coordinates```. 
    - ```world frame -> camera frame```. These are known as **extrinsic parameters**. R is the rotation matrix ```world->camera```, t is the translation ```world->camera```.  
        <p align="center">
        <img src="https://user-images.githubusercontent.com/39393023/147513029-9abe06fc-9025-4007-9d7c-fa55e26ed607.png" height="100" width="width"/>
        </p>
    - ```camera frame -> pixel coordinates```, where ```c_x, c_y``` are offset between the pixel frame and the camera frame. ```f_x, f_y``` are the focal lengths in x, y direcitions，**in pixels/meter**. ```Z_c``` is the depth of the object in the camera frame. This equation can be achived through similar triangle.  
        <p align="center">
        <img src="https://user-images.githubusercontent.com/39393023/147513027-0ae6d242-98fd-4abf-afba-b200c982d48d.png" height="80" width="width"/>
        </p>

        - To write in homogenous coordinates, we have: (```s=Zc```) 
            <p align="center">
            <img src="https://user-images.githubusercontent.com/39393023/147513030-e122029f-5a06-4484-863e-a159bb799edb.png" height="100" width="width"/>
            </p>
        - In [the OpenCV implementation](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html), distortion is added on top of ```X_c, Y_c``` **before** calculating [u, v]. Here for simplicity we skip that.  
    - All together, this is also called a homography (单应), which describes the relationship between the world frame and pixel coordinates. Note that we introduce the skew factor between teh x and y axes $\gamma$, which is in general close to 0.    
        $$ 
        s *\begin{bmatrix} u\\ v\\ 1 \end{bmatrix}
        = 
        \begin{bmatrix} f_x & \gamma& c_x\\ 0& f_y& c_y\\ 0& 0& 1\end{bmatrix}
        \begin{bmatrix}r_1& r_2& r_3& t\end{bmatrix}
        \begin{bmatrix}X_w\\ Y_w\\ Z_w\\ 1\end{bmatrix}
        $$
    - Conventionally, we define the intrinsics matrix as $M$
        $$
        M = \begin{bmatrix} f_x & \gamma & c_x\\ 0& f_y& c_y\\ 0& 0& 1\end{bmatrix}
        $$

## Calibration
A widely used calibration method was [proposed by Zhengyou Zhang in 1998 when he was at Microsoft](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr98-71.pdf). This method compared to its predecessors, requires simpler apparatus: just one checkerboard is enough. Although later, visual fiducial tags such as AprilTags and ArUco markers were invented and could provide better accuracy, for now, let's stick to the checkerboard. 

Now, in order to estimate the intrinsics and extrinsics, we need to find some points in the world frame (a.k.a object points), and their coordinates in the pixel frame (a.k.a imag points).   

1. **If we simplify the above model by assuming zw = 0**, then one can write it with a homography matrix, $H$, which describes the relationship between the same scene on two planes. Here ```s = Z_c```.
    $$
    \begin{bmatrix}u\\v\\1\end{bmatrix} 
    = \frac{1}{s}
    \begin{bmatrix}h_{11}&h_{12}&h_{13}\\ h_{21}&h_{22}&h_{23}\\ h_{31}&h_{32}&h_{33}\end{bmatrix}
    \begin{bmatrix}X_w\\ Y_w\\ 1\end{bmatrix}
    = \frac{1}{s}
    \begin{bmatrix} f_x & \gamma & c_x\\ 0& f_y& c_y\\ 0& 0& 1\end{bmatrix}
    \begin{bmatrix}r_1& r_2& t\end{bmatrix}
    \begin{bmatrix}X_w\\ Y_w\\ Z_w\\ 1\end{bmatrix}

    $$
    - Because we are working with homogenous coordinates, 1 out of ```H```'s 9 degrees of freedom is the scale factor, which yields the same result no matter how we change it. Therefore, ```H``` has 8 dof. 
    - Therefore, each pair of (object points, image points) will yield 2 equations, which means we need at least 4 such pairs to solve for all 8 equations:   
        $$
        \frac{h_{11}X_w + h_{12}Y_w + h_{13}}{h_{31}X_w+h_{32}Y_w+h_{33}} = u
        $$
        $$
        \frac{h_{21}X_w+h_{22}Y_w+h_{23}}{h_{31}X_w+h_{32}Y_w+h_{33}} = v
        $$

2. For ```n``` points, the above equations can be written as with $h$ defined below: 
    $$
    Ah =
    \begin{bmatrix}X_{1w}& Y_{1w}& 1& 0& 0& 0& -uX_{1w}& -uY_{1w}& -u\\
            
             0& 0& 0& X_{1w}& Y_{1w}& 1& -vX_{1w}& -vY_{1w}& -v& \\
    \vdots& \vdots& \vdots& \vdots& \vdots& \vdots& \vdots& \vdots& \vdots
    \end{bmatrix}
    \begin{bmatrix}h_{11}\\h_{12}\\h_{13}\\h_{21}\\h_{22}\\h_{23}\\h_{31}\\h_{32}\\h_{33}\end{bmatrix}
    =
    \begin{bmatrix}0\\ \vdots\\ 0\\\end{bmatrix}
    $$
    - Since we have 8 degrees of freedom, we can apply a constraint on the ```H``` matrix, such as let ```h_{31} = 1```, or let the squared sum of H $|h| = 1$. Then, **the optimization goal is to find H such that |Ah| is minimized.**
    - Due to noises, say our $(u,v)$ are off py a few pixels, estimating with 4 points will yield a lot of errors. So we need usually more than 10 points. 

### How do we solve for ```H```? 
1. Direct Linear Calibration (simple but not preferred)
    1. First, calculate the rank of $A$: $rank(A) = k$, then apply Singular Value Decomposition (SVD) on $A$: 
        $$
        A = UDV^T
        $$
       Note: 
         - $U$ is composed of the unit Eigenvectors of $AA^T$ (```mxm```) 
         - $V$ is composed of the unit Eigenvectors of $A^TA$ (```nxn```)
         - $D$ is a diagonal matrix (```kxk```) that contains square roots of $A^TA$'s eigen values: $\frac{1}{\sqrt(\lambda_i)}$, in decreasing order       
    2. Because $U$ is an [orthonormal basis](https://en.wikipedia.org/wiki/Orthonormality) of $R^m$ space, for any $x$, its length is preserved: $|Ux| = |x|$. So $min(|Ah|) = min(|UDV^Th|) = min(|DV^Th|)$. Because $D$ is a diagonal matrix with diagonal elements in decresing order, for any vector $y$, $min|Dy|$ is achieved when $y = (0,0, \cdots, 1)$. That means $min(|DV^Th|)$ is achieved when $V^Th = (0,0, \cdots, 1)$, and that is $h = last\_column(V)$ 
    3. Pros and Cons: 
        - Pros: simple, 
        - Cons: cannot enforce things like known focal length directly. More importantly, **the direct linear calibration method does not directly minimize the total reprojection error.** Hence, under noise this method is not as robust. That leads us to Levenberg-Marquardt optimization. 

2. Levenberg-Marquardt (LM) Optimization
    - The goal of LM Optmization is to minimze the total reprojection error - the total error between actual pixel coordinates and projected pixel coordinates of all 3D points. 
        $$
        argmin \sum_i(|\begin{bmatrix}u\\v\\1\end{bmatrix} - 
        H
        \begin{bmatrix}X_i\\Y_i\\1\end{bmatrix}
        |^2)
        $$
    - Flatten the above into the total sum of ```u,v``` directions. Then the objective function is ready for a LM optimizer
    - LM Optimization is a fascinating combination of Steepest Gradient Descent (SGD) and Newton-Gauss Method. [This article](https://medium.com/@sarvagya.vaish/levenberg-marquardt-optimization-part-1-981f5777b1d7) has some intuitive explanation on it.   

### How to solve for intrinsincs and extrinsics? 
1. At this step, we already solved for homography $H$ of a single camera view.     
    Rewrite $H$, 
    $$
    H = \frac{1}{s}
    M 
    \begin{bmatrix}r_1& r_2& t\end{bmatrix} = 
    \begin{bmatrix}h_1& h_2& h_3\end{bmatrix}
    $$
    
    We can get 
    $$
    r_1 = sM^{-1}h_1
    \\
    r_2 = sM^{-1}h_2
    \\
    t = sM^{-1}h_3
    $$
    
    Define $B$
    $$
    B = (M^{-1})^TM^{-1}
    $$
    Since $r_1$, $r_2$ comes from a rotation matrix, we can develop two equations from one homography $H$: 
    $$
    r_1^Tr_2 = 0 \rarr h_1^TBh_2 = 0
    \\
    r_1^Tr_1 = r_2^Tr_2 \rarr h_1^TBh_1 - h_2^TBh_2 = 0
    $$

2. Then we write out the "flattened version" of the above equation:  
    $$h_i^TBh_i = \bar{h}\bar{b}$$. 
   Little exercise - What exactly are $\bar{h}$ and $\bar{b}$? 

   Since $B$ is a **symmetric 3x3** matrix, we have 6 free variables. **So we need at least 3 distinct $H$**, which corresponds to 3 different camera views. Then,  we can solve for $\bar{b}$ by using similar tricks (SVD, LM optimization). 

3. After $B$ is solved, as an exercise, one can solve for **intrinsics** given that $B = (M^{-1})^TM^{-1}$. 

4. With $M$ solved, we can write out extrinsics $r_1, r_2, t$ as defined above. Also, we can complete the rotation matrix with $r_3 = r_1 \times r_2$. 


## Actual Implementation 
For beginners, OpenCV has encapsulated the entire calibration process really well. The actual implementation using OpenCV is pretty straightfoward. So, I highly recommend reading [their documentation](https://docs.opencv.org/3.4/dc/dbb/tutorial_py_calibration.html) before advancing to the rest of this section. 

In this section, I will present some notes and gotchas that I found useful for using OpenCV's API.

1. Present a chessboard and detect it. A chessboard can give us multiple pairs of points at the same time. We are able to assign each "effective" point to its world coordinates.  
    ![](https://docs.opencv.org/3.4/calib_pattern.jpg)
    1. Convert to scale image ```gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)```
        - This is important because chessboard internally checks for the "田" pattern of black and white squares. 
    2. find corners from the chessboard ```ret, corners = cv.findChessboardCorners(gray, (7,6), None)```
        - Note, the ```findChessboardCorners``` function looks for exactly 
    ```(ROW,COLUMN)``` number of squares. Also, the outermost layer of squares do not count. 
        - The more white space there is around the checker board, the more contrast there is. That will help with segmenting black squares. 
    3. Achieve sub-pixel accuracy: ```corners2 = cv.cornerSubPix(gray,corners, (11,11), (-1,-1), criteria)```
    4. Visualize the checker board. The board starts from the red line: 
    ```cv2.drawChessboardCorners(img, (7,6), corners2, ret)```
    5. Save multiple views. As we have shown, we need at least 3 pictures (the more, the better in practice). If we have less than 3, OpenCV will assume parameters such as $\gamma$ to be 0.

2. Calibration 
    1. Assign real world coordinates to chessboard. 
        - In this step, note that we assume ```z_w=0```. This is coherent to the assumption made in the previous calibration section. 
        - For each point, its 3D coordinates go into the ```objectPoints``` list, and its 2D pixel coordinates go into the ```imagePoints``` list. We want to make sure the correspondence is the same in these two lists (or vectors for C++). 
    2. Solve for intrinsics and extrinsics 
        ```python
        ret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(objectPoints, imagePoints, shape_of_grayscale_image, None, None)
        ```
        - ```ret``` is the total squared reprojection error over all points (as defined in the previous section). Usually good calibration should have this below 10. 
        - ```mtx``` is the intrinsics matrix $M$. ```f_x```, ```f_y``` in this matrix are in pixels/meter, so they may look bigger than we expected. 
        - ```dist``` measures the distortion of the image 
        - ```rvecs```, ```tvecs``` are the extrinsics that correspond to each camera view. 
        - For those who are curious, ```cv2.calibrateCamera``` uses LM optimization instead of the direct linear method. 
    3. For cameras with a wide field of view, you can consider using ```cv2.fisheye.calibrate```. The biggest difference here is the distortion model. However, for non-fisheye regular cameras, most likely ```cv2.calibrateCamera``` works better than ```fisheye.calibrate```. ```cv2.calibrateCamera``` already considers barrel and pincushion distortions.

3. Check Calibrated Result
    1. Calculate ```new_camera_mtx```, as if there was no distortion. 
        ```python
        h,  w = img.shape[:2]
        newcameramtx, roi = cv.getOptimalNewCameraMatrix(mtx, dist, (w,h), 1, (w,h))
        ```
        - [This post](https://answers.opencv.org/question/101398/what-does-the-getoptimalnewcameramatrix-function-does/) provides an intuitive explanation of this process: based on the distortion parameters, we want to "push" pixels to where they should be. 
    2. Undistort the image 
        ```python
        undistorted_frame = cv2.undistort(frame, self.params["mtx"], self.params["dist"], None, new_camera_mtx)
        ```
        - ```cv2.undistort``` is composed of ```cv2.initUndistortRectifyMap``` and ```cv2.remap```. 
        - ```initUndistortRectifyMap``` finds the mappings ```undistorted (u,v) -> original (u,v)``` along ```x,y``` directions. The exact mathematical representation can be found in the [OpenCV documentation](https://docs.opencv.org/3.4/da/d54/group__imgproc__transform.html#ga7dfb72c9cf9780a347fbe3d1c47e5d5a)
        - ```cv2.remap``` will directly apply mapping on the source image to build the output distorted image. 
            $$
            dst(u,v) = src(map_x(u,v), map_y(u,v))
            $$
        - If we have many images to undistort, it's recommended to compute ```cv2.initUndistortRectifyMap``` once, then apply the mappings on each image.
    


## Stereo Calibration 
Calibrate a pair of stereo cameras is to **generate a good homography between the two cameras**, i.e, the relative position between the two cameras. This information is crucial to depth estimation from a pair of stereo images. If you haven't done so, please check out my previous article on calibration for a single camera. From here, we assume that we have achieved a good set of extrinsics and intrinsics of the right and left cameras: ```mtx``` (camera intrinsics) and ```dist``` (distorsion)
### Theory 
To visualize what do with the homography generated in stereo calibration, given a pair of uncalibrated stereo images
    <p align="center">
    <img src="https://images2015.cnblogs.com/blog/810956/201602/810956-20160218201509534-648669511.png" height="400" width="width"/>
    <figcaption align="center">Image source: [CSDN](https://blog.csdn.net/weixin_39116058/article/details/85765543)</figcaption>
    </p>

With homography we are able to "align" the two cameras against the same plane
    <p align="center">
    <img src="https://images2015.cnblogs.com/blog/810956/201602/810956-20160218201746909-1138314477.png" height="400" width="width"/>
    <figcaption align="center">Image source: [CSDN](https://blog.csdn.net/weixin_39116058/article/details/85765543)</figcaption>
    </p>

The process to use homography for projecting image points onto the aligned image planes is called "image rectification". After this process, the two rectified camera views might be somewhat distorted, but their [epipolar lines](https://en.wikipedia.org/wiki/Epipolar_geometry) are parallel. This allows us to find matching points easily - we simply need to search for matching points along the epipolar lines/ 
    <p align="center">
    <img src="https://user-images.githubusercontent.com/77752418/153250025-50d3d840-fa97-45ef-9f33-1c414ff006b1.png" height="400" width="width"/>
    <figcaption align="center">Image source [CSDN](https://cloud.tencent.com/developer/article/1811227)</figcaption>
    </p>

Then to achieve the rotation and translation from left camera to right camera: 
1. Assume we have point P's coordinates in the left and right camera frames, $p_l, p_r$, and left & right camera frames rotation and translation from the world frame $R_l, T_l$, $R_r, T_r$ (from the single camera calibration process), we can get the transformation from left to right camera views $R_{rl}, T_{rl}$
    $$
    p_l = R_lp_w + T_l, 
    p_r = R_rp_w + T_r
    \\
    =>
    \\
    p_r = R_r(R_l^{-1}(p_l - T_l)) + T_r
    \\
    R_l^{-1} = R_l^T
    \\
    => 
    \\
    p_r = R_rR_l^{T}P_l + (T_l - R_rR_l^TT_l)
    \\
    => 
    \\
    R_{rl} = R_rR_l^{T}, T_{rl} = T_l - R_rR_l^TT_l
    $$




Reference: 
- nice derivation: https://cloud.tencent.com/developer/article/1811227
- summary: https://blog.csdn.net/weixin_30225755/article/details/112497742
### Code

1. Load parameters for left and right cameras:

2. The code I use is this (reference: [Temuge Batpurev's Blog](https://temugeb.github.io/opencv/python/2021/02/02/stereo-camera-calibration-and-triangulation.html))
    ```python
    # change this if this is not good
    stereocalibration_criteria = (cv2.TERM_CRITERIA_MAX_ITER + cv2.TERM_CRITERIA_EPS, 100, 0.001)
    RMSE,CM_left, dist_left, CM_right, dist_right, R, T, E, F = cv2.stereoCalibrate(
            obj_points, 
            img_points_left, img_points_right, 
            mtx_left, dist_left, 
            mtx_right, dist_right, 
            (width, height), 
            criteria = stereocalibration_criteria, 
            flags = cv2.CALIB_USE_INTRINSIC_GUESS
            )
    ```
    - Return values: RMSE (total root mean square error? TODO), camera matrix (unchanged), C1 distortion coefficients, C2 camera matrix (unchanged), C2 distortion coefficients, rotation matrix, translation vector, essential matrix and fundamental matrix. 
    - **Transformation in this case is from left camera to the right camera** $R_rl, T_rl$
    - From above, we can see that to solve for **stereo calibration**, we need not only intrinsics, but also extrinsics in each ```object_points -> image points```. In opencv, ```stereoCalibrate``` will first estimate the homography between each ```object_points -> image points```, then extrinsics can be solved easily (see the single camera calibration section). 
    - In ```cv2.stereoCalibrate()```,
        1. For each camera, solve for intrinsics if necessary (```cvCalibrateCamera2```)
        2. Calculate $R_l$, $R_r$, $T_l$, $T_r$ using ```cvFindExtrinsicCameraParams2``` (First use DLT method to find homography, then find extrinsics by plugging intrinsics in.)
        3. **The key is to have object points that correponds well to image points from left and right cams**


## 3D Point Cloud Generation Using Depth Estimation 

## Trilateration 
![](https://www.tothenew.com/blog/wp-content/uploads/2015/08/index.jpeg)

Let the prediction of person's location be $\vec{X}$, and we have 3 beacons at locations $\vec{A}$, $\vec{B}$, $\vec{C}$. Range measurements ```beacon->person``` are $a$, $b$, $c$ respectively 
$$
        H = 
        \begin{bmatrix}
        (\vec{X} - \vec{A})^T(\vec{X} - \vec{A}) - a^2 \\
        (\vec{X} - \vec{B})^T(\vec{X} - \vec{B}) - b^2 \\
        (\vec{X} - \vec{C})^T(\vec{X} - \vec{C}) - c^2 \\
        \end{bmatrix}
        =
        \begin{bmatrix} 0\\ 0\\ 0 \end{bmatrix}
$$

To solve for ```X```, we can use Gauss-Newton Method. Given the initial guess of ```X0```  
1. Calculate the difference between the predicted range and the actual range measurements
        $$
        H_0 = 
        \begin{bmatrix}
        (\vec{X_0} - \vec{A})^T(\vec{X_0} - \vec{A}) - a^2 \\
        (\vec{X_0} - \vec{B})^T(\vec{X_0} - \vec{B}) - b^2 \\
        (\vec{X_0} - \vec{C})^T(\vec{X_0} - \vec{C}) - c^2 \\
        \end{bmatrix}
        $$
2. Calculate Jacobian ```J```
$$
    J = \frac{\partial{H}}{\partial{\vec{X}}} = 
        \begin{bmatrix}
        2(\vec{X} - \vec{A})^T\\
        2(\vec{X} - \vec{B})^T\\
        2(\vec{X} - \vec{C})^T\\
        \end{bmatrix}
$$
3. Calculate step_size for $\vec{X}$, $\Delta{X}$
$$
    \Delta{X} = -(J^TJ)^{-1}J^TH_0
$$
4. Update $\vec{X} = \vec{X} + \Delta{X}$, which will be used as $\vec{X_0}$ for the next iteration. Repeat the above steps for a few more iterations until $H \rightarrow 0$
