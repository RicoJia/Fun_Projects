## From Ground Zero to 3D Point Cloud Using Stereo Camera
3D point cloud is widely used in autonomous driving, construction, augmented reality, etc. The 3D point cloud is best generated by a 3D Lidar. However for regular hobbyist projects, RGBD cameras, even a stereo camera, can give us some good estimations of the 3D environment. 

In this article, I am going to demonstrate this process using a dual lens stereo camera: 
    <p align="center">
    <img src="https://user-images.githubusercontent.com/39393023/149461165-9beba72c-421c-4570-a25a-9d6fa6e7b230.png" height="400" width="width"/>
    <figcaption align="center">SVPRO dual lens camera</figcaption>
    </p>

The major steps include: calibrating each lens as a single camera, calibrate two lenses as a stereo system, disparity image generation, and finally, estimating the 3D position of each pixel on the disparity image. 
    <p align="center">
    <img src="https://user-images.githubusercontent.com/39393023/149460886-7ce3a75c-ad28-4818-a088-ac1586cdf33f.png" height="800" width="width"/>
    <figcaption align="center">General code work flow - calibration and depth image</figcaption>
    </p>

### Single Camera Calibration 
Before stereo calibration, first we need to calibrate each lens. There are different types of camera models, such as for a fish eye camera (with large FOV), a good model is the [MEI](https://www.robots.ox.ac.uk/~cmei/articles/single_viewpoint_calib_mei_07.pdf), currently adopted by [OpenCV](https://docs.opencv.org/3.4/dd/d12/tutorial_omnidir_calib_main.html). In this article, we are going to use the traditional pinhole camera model. 

#### Pinhole Model 
The common pinhole model is set up as below. $f$ is focal length. The image will be projected onto the plane behind the lens, in an angle $\phi$
      <p align="center">
      <img src="https://user-images.githubusercontent.com/39393023/121839683-3d499c00-cca0-11eb-8faf-22485a5248e1.png" height="200" width="Field of View"/>
      <figcaption align="center">Pinhole Model. Credit: Udacity</figcaption>
      </p>

In the above depiction, note that the projection of an object through pinhole is upside down. For convenience, in 3D we shift the image plane **before** the pinhole, and we set up the camera frame at the pinhole. This way, camera frame coordinates on the image plane can be easily calculated from the 3D world frame coordinates using the similar triangle method, without flipping them upside down. 
    <p align="center">
    <img src="https://docs.opencv.org/4.x/pinhole_camera_model.png" height="200" width="width"/>
    <figcaption align="center">3D Pinhole Model. Credit: OpenCV</figcaption>
    </p>

Here we usually represent coordinates using homogenous coordinates, which allows us to represent depth ```z``` very easily in matrix form. This is important because the same point on the image plane can be the projection of points of different depth.    

1. The model of a simple pinhole camera can be written as:  ```world frame -> camera frame -> pixel coordinates```. 
    - ```world frame -> camera frame```. These are known as **extrinsic parameters**. R is the rotation matrix ```world->camera```, t is the translation ```world->camera```.  
        <p align="center">
        <img src="https://user-images.githubusercontent.com/39393023/147513029-9abe06fc-9025-4007-9d7c-fa55e26ed607.png" height="100" width="width"/>
        </p>
    - ```camera frame -> pixel coordinates```, where ```c_x, c_y``` are offset between the pixel frame and the camera frame. ```f_x, f_y``` are the focal lengths in x, y direcitions，**in pixels/meter**. ```Z_c``` is the depth of the object in the camera frame. This equation can be achived through similar triangle.  
        <p align="center">
        <img src="https://user-images.githubusercontent.com/39393023/147513027-0ae6d242-98fd-4abf-afba-b200c982d48d.png" height="80" width="width"/>
        </p>

        - To write in homogenous coordinates, we have: (```s=Zc```) 
            <p align="center">
            <img src="https://user-images.githubusercontent.com/39393023/147513030-e122029f-5a06-4484-863e-a159bb799edb.png" height="100" width="width"/>
            </p>
        - In [the OpenCV implementation](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html), distortion is added on top of ```X_c, Y_c``` **before** calculating [u, v]. Here for simplicity we skip that.  
    - All together, this is also called a homography (单应), which describes the relationship between the world frame and pixel coordinates. 
        $$ 
        s *\begin{bmatrix} u\\ v\\ 1 \end{bmatrix}
        = 
        \begin{bmatrix} f_x & 0 & c_x\\ 0& f_y& c_y\\ 0& 0& 1\end{bmatrix}
        \begin{bmatrix}r_1& r_2& r_3& t\end{bmatrix}
        \begin{bmatrix}X_w\\ Y_w\\ Z_w\\ 1\end{bmatrix}
        $$

### Calibration
A widely used calibration method was [proposed by Zhengyou Zhang in 1998 when he was at Microsoft](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr98-71.pdf). This method compared to its predecessors, require simpler apparatus: just one checkerboard is enough. Although later, visual fiducial tags such as aprilTags and ArUco were invented and could provide better accuracy, for now, let's stick to the checkerboard. 

Now, in order to estimate the intrinsics and extrinsics, we need to find some points in the world frame (a.k.a object points), and their coordinates in the pixel frame (a.k.a imag points).   

1. **If we simplify the above model by assuming zw = 0**, then one can write it with a homography matrix, $H$, which describes the relationship between the same scene on two planes. Here ```s = Z_c```.
    $$
    \begin{bmatrix}u\\v\\1\end{bmatrix} 
    = \frac{1}{s}
    \begin{bmatrix}h_{11}&h_{12}&h_{13}\\ h_{21}&h_{22}&h_{23}\\ h_{31}&h_{32}&h_{33}\end{bmatrix}
    \begin{bmatrix}X_w\\ Y_w\\ 1\end{bmatrix}
    $$
    - Because we are working with homogenous coordinates, 1 out of ```H```'s 9 degrees of freedom is the scale, which yields the same result no matter how we change it. Therefore, ```H``` has 8 dof. 
    - Therefore, each pair of (object points, image points) will yield 2 equations, which means we need at least 4 such pairs to solve for all 8 equations:   
        $$
        \frac{h_{11}X_w + h_{12}Y_w + h_{13}}{h_{31}X_w+h_{32}Y_w+h_{33}} = u
        $$
        $$
        \frac{h_{21}X_w+h_{22}Y_w+h_{23}}{h_{31}X_w+h_{32}Y_w+h_{33}} = v
        $$


#### Sketch[]
2. 
3. Code: Take another look



========================================================================
## Calibration 
========================================================================



2. Calibration Derivation Version . 
    2. Homography has 8 free variables, 
        - because it's homogenous coordinates, we can have everything times k, and they're still valid! So we can put a constraint (e.g., h33 = 1, or |H| = 1) , and can be solved with at least 4 pairs.
        <p align="center">
        <img src="https://user-images.githubusercontent.com/39393023/147690865-99983dec-f257-4c8f-a219-aaed6b80e0b6.JPEG" height="400" width="width"/>
        </p>
        
        - SVD (using a different notation)
          <p align="center">
          <img src="https://user-images.githubusercontent.com/39393023/122309167-6736d480-ced3-11eb-806f-6dcf7928dc40.jpeg" height="600" width="width"/>
          <figcaption align="center">Calibration set up and SVD</figcaption>
          </p>

    3. With H solved, we can get two eqns for solving for intrinsics, M. M is a symmetric matrix, so it has 6 variables, and requires 6/2=3 different views. 
        <p align="center">
        <img src="https://user-images.githubusercontent.com/39393023/147690866-e5bd752c-2061-4293-bcce-c900e5e22db6.JPEG" height="600" width="width"/>
        </p>
    
3. How does calibrateCamera really calculate M (intrinsics) and [r1 r2 t] (extrinsics), and k (distorsion)? Using LM Optimiztion 
    - It tries to minimize the total reprojection error
        <p align="center">
        <img src="https://user-images.githubusercontent.com/39393023/147775423-53fee81d-f66d-460f-a36a-d16989d4284d.JPEG" height="400" width="width"/>
        </p>

4. Calibration under distortion, [undistorsion](https://codeantenna.com/a/4GFy7ZovEP)
    - Distortion
      <p align="center">
      <img src="https://clickitupanotch.com/wp-content/uploads/2014/06/lens-distortion-graphic.jpg" height="200" width="width"/>
      <figcaption align="center">Distortion</figcaption>
      </p>
    - requires non-linear optimization
    - caused by light being bent more near the edges of a lens. 
    - barrel distortion is "center appears larger than edges"
    - There's also tangential Distortion. 

