## From Ground Zero to 3D Point Cloud Using Stereo Camea
3D point cloud is widely used in autonomous driving, construction, augmented reality, etc. The 3D point cloud is best generated by a 3D Lidar. However for regular hobbyist projects, RGBD cameras, even a stereo camera, can give us some good estimations of the 3D environment. 

In this article, I am going to demonstrate this process using a dual lens stereo camera: 
    <p align="center">
    <img src="https://user-images.githubusercontent.com/39393023/149461165-9beba72c-421c-4570-a25a-9d6fa6e7b230.png" height="400" width="width"/>
    <figcaption align="center">SVPRO dual lens camera</figcaption>
    </p>

The major steps include: calibrating each lens as a single camera, calibrate two lenses as a stereo system, disparity image generation, and finally, estimating the 3D position of each pixel on the disparity image. 
    <p align="center">
    <img src="https://user-images.githubusercontent.com/39393023/149460886-7ce3a75c-ad28-4818-a088-ac1586cdf33f.png" height="800" width="width"/>
    <figcaption align="center">General code work flow - calibration and depth image</figcaption>
    </p>

### Single Camera Calibration 
#### Principle 
Before stereo calibration, first we need to calibrate each lens. There are different types of camera models, such as for a fish eye camera (with large FOV), a good model is the [MEI](https://www.robots.ox.ac.uk/~cmei/articles/single_viewpoint_calib_mei_07.pdf), currently adopted by [OpenCV](https://docs.opencv.org/3.4/dd/d12/tutorial_omnidir_calib_main.html). In this article, we are going to use the traditional pinhole camera model. 

#### Pinhole Model 
The common pinhole model is set up as below. $f$ is focal length. The image will be projected onto the plane behind the lens, in an angle $\phi$
      <p align="center">
      <img src="https://user-images.githubusercontent.com/39393023/121839683-3d499c00-cca0-11eb-8faf-22485a5248e1.png" height="200" width="Field of View"/>
      <figcaption align="center">Pinhole Model. Credit: Udacity</figcaption>
      </p>

In the above depiction, note that the projection of an object through pinhole is upside down. For convenience, in 3D we shift the image plane **before** the pinhole, and we set up the camera frame at the pinhole. This way, camera frame coordinates on the image plane can be easily calculated from the 3D world frame coordinates using the similar triangle method, without flipping them upside down. 
    <p align="center">
    <img src="https://docs.opencv.org/4.x/pinhole_camera_model.png" height="300" width="width"/>
    </p>



#### Sketch
pinhole

Zhengyou 

math 
2. What the function does 
[]
3. Code: Take another look



========================================================================
## Calibration 
========================================================================
### Mono Camera Calibration
1. calibrateCamera: no skew param in intrinsics. [Good explanation](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html). **The goal of calibration is to know the 6 extrinsics and 5 intrinsics, and skew params** [从零开始学习「张氏相机标定法」](https://mp.weixin.qq.com/s/48jMBVVEqQp3IR6NpUy2QA)

    - three frames: world frame, camera frame (at the shutter), and image frame. 
        - world frame -> camera frame
            <p align="center">
            <img src="https://user-images.githubusercontent.com/39393023/147513029-9abe06fc-9025-4007-9d7c-fa55e26ed607.png" height="100" width="width"/>
            </p>

        - camera_frame -> image_frame: intrinsics. So if we know world frame, we know its image coord. But we can't go the other way
            <p align="center">
            <img src="https://user-images.githubusercontent.com/39393023/147513030-e122029f-5a06-4484-863e-a159bb799edb.png" height="100" width="width"/>
            </p>

            <p align="center">
            <img src="https://user-images.githubusercontent.com/39393023/147513027-0ae6d242-98fd-4abf-afba-b200c982d48d.png" height="100" width="width"/>
            </p>

2. Calibration Derivation Version . 
    1. Homography of one picture (单应) is the ```world -> (u,v)```. Note we assume zw = 0 as zc (z in the triangle) is calculated. 
        <p align="center">
        <img src="https://user-images.githubusercontent.com/39393023/147690862-fb9f5d49-0cb8-4d4f-b4c3-5b172cc35af4.JPEG" height="300" width="width"/>
        </p>

    2. Homography has 8 free variables, because it's homogenous coordinates, we can have everything times k, and they're still valid! So we can put a constraint (e.g., h33 = 1, or |H| = 1) , and can be solved with at least 4 pairs.
        <p align="center">
        <img src="https://user-images.githubusercontent.com/39393023/147690865-99983dec-f257-4c8f-a219-aaed6b80e0b6.JPEG" height="400" width="width"/>
        </p>
        
        - SVD (using a different notation)
          <p align="center">
          <img src="https://user-images.githubusercontent.com/39393023/122309167-6736d480-ced3-11eb-806f-6dcf7928dc40.jpeg" height="600" width="width"/>
          <figcaption align="center">Calibration set up and SVD</figcaption>
          </p>

    3. With H solved, we can get two eqns for solving for intrinsics, M. M is a symmetric matrix, so it has 6 variables, and requires 6/2=3 different views. 
        <p align="center">
        <img src="https://user-images.githubusercontent.com/39393023/147690866-e5bd752c-2061-4293-bcce-c900e5e22db6.JPEG" height="600" width="width"/>
        </p>
    
3. How does calibrateCamera really calculate M (intrinsics) and [r1 r2 t] (extrinsics), and k (distorsion)? Using LM Optimiztion 
    - It tries to minimize the total reprojection error
        <p align="center">
        <img src="https://user-images.githubusercontent.com/39393023/147775423-53fee81d-f66d-460f-a36a-d16989d4284d.JPEG" height="400" width="width"/>
        </p>

4. Calibration under distortion, [undistorsion](https://codeantenna.com/a/4GFy7ZovEP)
    - Distortion
      <p align="center">
      <img src="https://clickitupanotch.com/wp-content/uploads/2014/06/lens-distortion-graphic.jpg" height="200" width="width"/>
      <figcaption align="center">Distortion</figcaption>
      </p>
    - requires non-linear optimization
    - caused by light being bent more near the edges of a lens. 
    - barrel distortion is "center appears larger than edges"
    - There's also tangential Distortion. 

